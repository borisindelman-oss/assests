model = "gpt-5.2-codex"
model_reasoning_effort = "high"
tool_output_token_limit = 25000
# Leave room for native compaction near the 272–273k context window.
# Formula: 273000 - (tool_output_token_limit + 15000)
# With tool_output_token_limit=25000 ⇒ 273000 - (25000 + 15000) = 233000
model_auto_compact_token_limit = 233000
# sandbox_mode = "danger-full-access"


[features]
ghost_commit = false
unified_exec = true
apply_patch_freeform = true
web_search_request = true
skills = true
shell_snapshot = true

[ghost_snapshot]
ignore_large_untracked_files = 104857600

[projects."/workspace/WayveCode"]
trust_level = "trusted"

[projects."/Users/borisindelman/git/vault"]
trust_level = "trusted"

[sandbox_workspace_write]
network_access = true

[mcp_servers.notion]
url = "https://mcp.notion.com/mcp"
startup_timeout_sec = 60

[mcp_servers.github]
command = "docker"
args = [
  "run",
  "-i",
  "--rm",
  "-e",
  "GITHUB_PERSONAL_ACCESS_TOKEN",
  "ghcr.io/github/github-mcp-server"
]

[mcp_servers.github.env]
GITHUB_PERSONAL_ACCESS_TOKEN = "${GITHUB_PERSONAL_ACCESS_TOKEN}"
